{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)\n",
    "#stratify: If not None, data is split in a stratified fashion, using this as the class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X, y = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scaller should be fitted only on train test (should not see the test set.)\n",
    "# then such trained scaller should be used to scale the test set. Otherwise the results\n",
    "# can be overpesymistic,  information from the test set will \"leak\" into your training data.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "#RobustScaler -> based on decades, less sensitive to outliers comared to StandardScaller\n",
    "scaler.fit(X_train)\n",
    "X_trans_train = scaler.transform(X_train)\n",
    "X_trans_test = scaler.transform(X_test)  #### Use this same scaler from train to transform test data\n",
    "# OR using fit_trnasform:\n",
    "#the transfomrers and scallers can be simplified, such that in one step it fits and rescales the data:\n",
    "X_trainRescaled=scaler.fit_transform(X_train)\n",
    "\n",
    "#function to handle missiong values\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "#Addind features:\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features (bag of word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import LabelEncoder, LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For text categorization (bilidning a dict):\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Word dict as above, but more frequent has lower weights:\n",
    "#with mninium ocurance in 5 documents, and sequence of singe and duplet words (for example: \"not issue\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectrorizedText = TfidfVectorizer(min_df =5, ngram_range(1,2)).fit(text_train) \n",
    "\n",
    "#other features:\n",
    "# Notice the column names and that DictVectorizer doesn’t touch (convert) numeric values.\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "new=vec.fit_transform(measurements)\n",
    "vec.get_feature_names()\n",
    "\n",
    "# OR using Pandas ##\n",
    "####### DROP FIRST = TRUE to avoide collinearity (redundancy):\n",
    "final_data = pd.get_dummies(loans, columns=cat_feats, drop_first=True)\n",
    "#using drop_first is important to avoid \"colinearity\" or overcolrelaiton of data (see below)\n",
    "# drop_first -   Whether to get k-1 dummies out of k categorical levels by removing the first level.\n",
    "features_dummies = pd.get_dummies(features, columns=['pclass', 'sex', 'embarked'])\n",
    "features_dummies.head(n=16)\n",
    "\n",
    "#important to remove redundant data. Like sex in Titalic data, ot remove \n",
    "#strong correlatio between dumines male/female. Rahter just keep one with 0/1 vales\n",
    "sex = pd.get_dummies(train['Sex'],drop_first=True)\n",
    "\n",
    "\n",
    "#below one if for binalization categories\n",
    "# Binarize labels in a one-vs-all fashion (includint ints numbers)\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#     encode categorical integer features using a one-hot aka one-of-K scheme.\n",
    "\n",
    "from sklearn import preprocessing\n",
    ">>> le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1.1.2  Model-based Feature Selection¶\n",
    "\n",
    "A somewhat more sophisticated method for feature selection is using a supervised machine learning model and selecting features based on how important they were deemed by the model. This requires the model to provide some way to rank the features by importance. This can be done for all tree-based models (which implement get_feature_importances) and all linear models, for which the coefficients can be used to determine how much influence a feature has on the outcome.\n",
    "\n",
    "Any of these models can be made into a transformer that does feature selection by wrapping it with the SelectFromModel class:\n",
    "    \n",
    "1.1.3  Recursive Feature Elimination\n",
    "\n",
    "Recursive feature elimination builds a model on the full set of features, and similar to the method above selects a subset of features that are deemed most important by the model. However, usually only a single feature is dropped from the dataset, and a new model is built with the remaining features. The process of dropping features and model building is repeated until there are only a pre-specified number of features left:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Supervised:\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC # similar to Logisitc regresion\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "model.predict()\n",
    "model.predict_proba()\n",
    "model.decision_function()\n",
    "model.score()\n",
    "model.transform()\n",
    "\n",
    "#Unsupervised:\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "%Nonlinear mainfolds (nonlinear fueatures in X set, unsupervised separation, like PCA)\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.manifold import DBSCAN # no need to specfify number of clusters, works well with noise\n",
    "\n",
    "\n",
    "\n",
    "model.transform()\n",
    "model.fit_transform()\n",
    "model.predict()\n",
    "model.predict_proba()\n",
    "model.score()\n",
    "\n",
    "# ## Manifold Learning\n",
    "# One weakness of PCA is that it cannot detect non-linear features.  A set\n",
    "# of algorithms known as *Manifold Learning* have been developed to address\n",
    "# this deficiency.  A canonical dataset used in Manifold learning is the\n",
    "# *S-curve*:\n",
    "    \n",
    "# Using a more powerful, nonlinear techinque can provide much better visualizations, though.\n",
    "# Here, we are using the t-SNE manifold learning method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usefull is fitting and predicting in single step:\n",
    "y_pred =  cls.fit_predict(X_train)\n",
    "#there is also finction\n",
    "X_train_trans=cls.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It is alos possible to make own processor, or calsifier.\n",
    "It just have to have a proper methods, like predict, transorm, fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr style=\"border:None; font-size:20px; padding:10px;\"><th>``model.predict``</th><th>``model.transform``</th></tr>\n",
    "<tr style=\"border:None; font-size:20px; padding:10px;\"><td>Classification</td><td>Preprocessing</td></tr>\n",
    "<tr style=\"border:None; font-size:20px; padding:10px;\"><td>Regression</td><td>Dimensionality Reduction</td></tr>\n",
    "<tr style=\"border:None; font-size:20px; padding:10px;\"><td>Clustering</td><td>Feature Extraction</td></tr>\n",
    "<tr style=\"border:None; font-size:20px; padding:10px;\"><td>&nbsp;</td><td>Feature Selection</td></tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge #L2 regularization\n",
    "from sklearn.linear_model import Lasso #L1 regularization\n",
    "\n",
    "ridge = Ridge(alpha=0.1).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Often only few samples have non-zero αα, these are called the \"support vectors\" from which SVMs get their name. These are the most discriminant samples.\n",
    "\n",
    "The most important parameter of the SVM is the regularization parameter CC, which bounds the influence of each individual sample:\n",
    "\n",
    "    Low C values: many support vectors... Decision frontier = mean(class A) - mean(class B)\n",
    "    High C values: small number of support vectors: Decision frontier fully driven by most discriminant samples\n",
    "\n",
    "            \n",
    "SVC(kernel=\"linear\") , linear, poly, rbf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load figures\\plot_svm_interactive.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_trainSc=scaler.fit_transform(X_train)\n",
    "X_testSc=scaler.transform(X_test)\n",
    "X_trainPCA=pca.fit_transform(X_trainSc)\n",
    "X_testPCA=pca.transform(X_testSc)\n",
    "\n",
    "Here it is useful to use a variant of PCA called ``RandomizedPCA``, which is an approximation of PCA that can be much faster for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisions Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "reg = DecisionTreeRegressor(max_depth=3)\n",
    "#max_depth is number of number of questions: 2^max_depth\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are fast to train, easy to understand, and often lead to interpretable models. However, single trees often tend to overfit the training data. Playing with the slider above you might notice that the model starts to overfit even before it has a good separation between the classes.\n",
    "\n",
    "Therefore, in practice it is more common to combine multiple trees to produce models that generalize better. The most common methods for combining trees are random forests and gradient boosted trees.\n",
    "\n",
    "Random Forests\n",
    "\n",
    "Random forests are simply many trees, built on different random subsets (drawn with replacement) of the data, and using different random subsets (drawn without replacement) of the features for each split. This makes the trees different from each other, and makes them overfit to different aspects. Then, their predictions are averaged, leading to a smoother estimate that overfits less.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#but it seems that random forest is superior to Boosting\n",
    "#Boostin is good if ther is no errors i labales!\n",
    "#Boosting is more prone to overfitting than bagging (Random forest), or decision tree\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "clf = GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=.2)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=i, \n",
    "                random_state=0)\n",
    "    km.fit(X)\n",
    "    distortions.append(km.inertia_)\n",
    "    \n",
    "#OR:\n",
    "y_pred = KMeans(n_clusters=3).fit_predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "regressor.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(est, X, y):\n",
    "    training_set_size, train_scores, test_scores = learning_curve(est, X, y, train_sizes=np.linspace(.1, 1, 20))\n",
    "    estimator_name = est.__class__.__name__\n",
    "    line = plt.plot(training_set_size, train_scores.mean(axis=1), '--', label=\"training scores \" + estimator_name)\n",
    "    plt.plot(training_set_size, test_scores.mean(axis=1), '-', label=\"test scores \" + estimator_name, c=line[0].get_color())\n",
    "    plt.xlabel('Training set size')\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "\n",
    "plt.figure()    \n",
    "plot_learning_curve(LinearRegression(), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "cross_val_score(classifier, X, y, cv=5)\n",
    "#Important: CV do not shufffle by defolut. Thuse important to shuffe data before\n",
    "#for example during \"train_test_split\"or use: shuffle (from sklearn.utils)\n",
    "# cv= in -> implise stratified splits (preserving the clases ration)\n",
    "# scoring for discreet (binary??) data\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "OR:\n",
    "from sklearn.metrics import metrics\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "metrics.classification_report(expected, predicted)\n",
    "\n",
    "print('MAE:',metrics.mean_absolute_error(y_test, pred))\n",
    "print('MSE:',metrics.mean_squared_error(y_test, pred))\n",
    "print('RMSE:',metrics.mean_squared_error(y_test, pred)**0.5)\n",
    "print('R^2 value: ', metrics.explained_variance_score(y_test, pred))\n",
    "\n",
    "OR:\n",
    "metrics.confusion_matrix(expected, predicted)\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "-> to compare the clasificiation labels, in unsupervised, regardless of the clases names,\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "Recall = TP / (TP + FN)\n",
    "F1 = 2 x (precision x recall) / (precision + recall)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "#roc - goves a ROC curve, roc_auc_score: is the AreaUnderCureve - single vale score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list of available scoring functions\n",
    "from sklearn.metrics.scorer import SCORERS\n",
    "print(SCORERS.keys())\n",
    "\n",
    "#own scoring function:\n",
    "def my_accuracy_scoring(est, X, y):\n",
    "    return np.mean(est.predict(X) == y)\n",
    "cross_val_score(SVC(), X, y, scoring=my_accuracy_scoring)\n",
    "#onw scorring, with acess to etimator parameters\n",
    "def my_super_scoring(est, X, y):\n",
    "    return np.mean(est.predict(X) == y) - np.mean(est.coef_ != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "For doing grid-search and cross-validation, we usually want to condense our model evaluation into a single number. A good way to do this with the roc curve is to use the area under the curve (AUC). We can simply use this in cross_val_score by specifying scoring=\"roc_auc\":\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "cross_val_score(SVC(), X, y, scoring=\"roc_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "cv = KFold(shuffle=True)  # But the data should be already shuffled\n",
    "#by train_test_split\n",
    "for train, test in cv.split(iris.data, iris.target):\n",
    "    print(test)\n",
    "#OR:\n",
    "for n_neighbors in [1, 3, 5, 10, 20]:\n",
    "    scores = cross_val_score(KNeighborsRegressor(n_neighbors=n_neighbors), X, y, cv=cv)\n",
    "    print(\"n_neighbors: %d, average score: %f\" % (n_neighbors, np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "n_neighbors = [1, 3, 5, 10, 20, 50]\n",
    "train_errors, test_errors = validation_curve(KNeighborsRegressor(), X, y, param_name=\"n_neighbors\",\n",
    "                                             param_range=n_neighbors, cv=cv)\n",
    "plt.plot(n_neighbors, train_errors.mean(axis=1), label=\"train error\")\n",
    "plt.plot(n_neighbors, test_errors.mean(axis=1), label=\"test error\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "cv = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "param_grid = {'C': 10**np.arrange(-3,3), 'gamma': [0.001, 0.01, 0.1, 1]}\n",
    "grid = GridSearchCV(SVR(), param_grid=param_grid, , scoring=\"roc_auc\", cv=cv, verbose=2)\n",
    "grid.fit(X, y)\n",
    "grid.predict(X)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random SearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Advnetage: the unimportant parameters, do not increase calculation time\n",
    "# as oposed to grid search. And it can be executed just for a given\n",
    "# number of iterations, lets say 100, and at any point, it waoul\n",
    "# alwasys explore the  full serach space (randomly). Whule grid serch must acoplisch\n",
    "# the whole serching, otherwise it can skip somethng.\n",
    "# If all parameters are presented as a list,\n",
    "# sampling without replacement is performed. If at least one parameter\n",
    "# is given as a distribution, sampling with replacement is used.\n",
    "# It is highly recommended to use continuous distributions for continuous\n",
    "# parameters.\n",
    "# param_distributions : dict\n",
    "#     Dictionary with parameters names (string) as keys and distributions\n",
    "#     or lists of parameters to try. Distributions must provide a ``rvs``\n",
    "#     method for sampling (such as those from scipy.stats.distributions).\n",
    "#     If a list is given, it is sampled uniformly.\n",
    "from scipy.stats import randint as sp_randint\n",
    "params = {'ParameterOne__X': [1,2,3], 'linearsvc__C': expon() OR::: sp_randint(1, 11)}\n",
    ":::OR: 'svm_c': expon(scale=100, loc=5)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rs = RandomizedSearchCV(text_pipe, param_distributions=params, n_iter=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is better to use Pipilien through the whole porcess.\n",
    "otherwise, even if there is scaller used on traingin data, then CV it is splited into tran and validation data, but scaalin already was affected by the test data- contaminated!!!\n",
    "\n",
    "...Here, we did grid-search with cross-validation on X_train. However, when applying TfidfVectorizer, it saw all of the X_train, not only the training folds! So it could use knowledge of the frequency of the words in the test-folds. This is called \"contamination\" of the test set, and leads to too optimistic estimates of generalization performance, or badly selected parameters. We can fix this with the pipeline, though\n",
    "**see below**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Better approach with Pipleine and GridSearch\n",
    "# Normalization should be done after data spliting\n",
    "\n",
    "# Train test split:\n",
    "X = df.drop('TARGET CLASS', axis=1)\n",
    "y = df['TARGET CLASS']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
    "\n",
    "#Make Pipline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "\n",
    "#But such piplie can be constructed over grid,to make a proper search\n",
    "param_grid = {'kneighborsclassifier__n_neighbors': range(1,30)}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(pipe, param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "bestK = grid.best_params_.get('kneighborsclassifier__n_neighbors')\n",
    "print('Best score = {}'.format(grid.score(X_test, y_test)))\n",
    "print('Best value of k = {}'.format(bestK))\n",
    "\n",
    "mean_test_score = grid.cv_results_.get('mean_test_score')\n",
    "mean_train_score = grid.cv_results_.get('mean_train_score')\n",
    "kn = grid.cv_results_.get('param_kneighborsclassifier__n_neighbors')\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(kn, mean_test_score, label = 'Test set')\n",
    "plt.plot(kn, mean_train_score, label = 'Train set')\n",
    "plt.xlabel('k neighbors')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is an example of the best practice of the workfolw (SciPy 2016, Muller)\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(MinMaxScaller(),SVC())\n",
    "pipe.fit(X_train, y_train).score(X_test, y_test)\n",
    "\n",
    "#But such piplie can be constructed over grid,to make a proper search\n",
    "param_grid = {'svc__C': 10**np.arrange(-3,3),\n",
    "             'svc__gamma':[0.0001,0.1, 10]}\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "grid = GridSearchCV(pipe, param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "pipeline = make_pipeline(TfidfVectorizer(), LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(TfidfVectorizer(), \n",
    "                         LogisticRegression())\n",
    "grid = GridSearchCV(pipeline,\n",
    "                    param_grid={'logisticregression__C': [.1, 1, 10, 100]}, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "grid.score(text_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template and usefull files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark  -d -u -a 'Pawel Wnuk' -v -p numpy,scipy,matplotlib,scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #for nicer data visualization\n",
    "%matplotlib inline\n",
    "# plt.style.use('bmh') style for pyplot matplotlib\n",
    "rnd = np.random.RandomState(seed=123)\n",
    "\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True # to have separation lines in histogram plots\n",
    "\n",
    "#Optional for DataFrames colors improvement\n",
    "from IPython.core.display import HTML\n",
    "css = open('style-table.css').read() + open('style-notebook.css').read()\n",
    "HTML('<style>{}</style>'.format(css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "picle - is a way to save a coniteiner of differnt data,ti reuse it later with pytho, (cluster, or data structure)\n",
    "also pandas has its onw pickle mechanism (to save data to file): df1.to_pickle('testfiele.pickle) - thats it\n",
    "\n",
    "> import shelve - also for pickling differnt types of data\n",
    "\n",
    "You learned two techniques that you can use:\n",
    "\n",
    "    The pickle API for serializing standard Python objects.\n",
    "    The joblib API for efficiently serializing Python objects with NumPy arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saving data/cassiiers from scikit: \n",
    "import joblib\n",
    "In the specific case of the scikit, it may be more interesting to use joblib’s replacement of pickle (joblib.dump & joblib.load), which is more efficient on objects that carry large numpy arrays internally as is often the case for fitted scikit-learn estimators, but can only pickle to the disk and not to a string:\n",
    "\n",
    ">>> from sklearn.externals import joblib\n",
    ">>> joblib.dump(clf, 'filename.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples in SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_....\n",
    "from sklearn.datasets import fetch...\n",
    "from sklearn.datasets import make_...\n",
    "\n",
    "from sklearn.datasets import get_data_home\n",
    "get_data_home()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import ranking\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "titanic = pd.read_csv(os.path.join('datasets', 'titanic3.csv'))\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = titanic.survived.values\n",
    "features = titanic[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
    "features.head()\n",
    "\n",
    "features_dummies = pd.get_dummies(features, columns=['pclass', 'sex', 'embarked'])\n",
    "pd.get_dummies(features).head()\n",
    "features_dummies.drop('sex_female',axis=1, inplace=True)\n",
    "features_dummies.rename(columns={'sex_male': 'sex'}, inplace=True)\n",
    "data = features_dummies.values\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, random_state=0)\n",
    "\n",
    "imp = Imputer()\n",
    "imp.fit(train_data)\n",
    "train_data_finite = imp.transform(train_data)\n",
    "test_data_finite = imp.transform(test_data)\n",
    "\n",
    "#Random forest:\n",
    "clfRF = RandomForestClassifier(n_estimators=100)\n",
    "clfRF.fit(train_data_finite, train_labels)\n",
    "\n",
    "pred_labels = clfRF.predict(test_data_finite)\n",
    "print(\"Prediction accuracy: %f\" % accuracy_score(pred_labels, test_labels))\n",
    "print('Number of features: ', len(clfRF.feature_importances_))\n",
    "\n",
    "#Logisitic Regresion:\n",
    "clfLR = LogisticRegression()\n",
    "clfLR.fit(train_data_finite, train_labels)\n",
    "\n",
    "pred_labels = clfLR.predict(test_data_finite)\n",
    "print(\"Prediction accuracy: %f\" % accuracy_score(pred_labels, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "keys = features_dummies.columns\n",
    "importances = clfRF.feature_importances_\n",
    "\n",
    "# print(importances)\n",
    "std = np.std([tree.feature_importances_ for tree in clfRF.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "a=6\n",
    "indices=indices[:a]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(a), importances[indices],\n",
    "       color=\"r\",  yerr = std[indices], align=\"center\")\n",
    "feat =  [keys[i] for i in indices]\n",
    "plt.xticks(range(a), feat)\n",
    "plt.xlim([-1, a])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ploting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.loria.fr/~rougier/teaching/matplotlib - A good matplotlib tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%matloptlilb notebook #interacive graphs, + controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nice info about parmaters, line styles, colors, etc:\n",
    "plt.plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ploting calsifier:\n",
    "from figures import plot_2d_separator\n",
    "plot_2d_separator(clf, X, fill=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# there are two differnt ways of plotting with matplotlib:\n",
    "# 1) Functional \n",
    "plt.plot(x, y, 'g*-') # 'r' is the color red\n",
    "plt.xlabel('X Axis Title Here')\n",
    "plt.ylabel('Y Axis Title Here')\n",
    "plt.title('String Title Here')\n",
    "plt.show() #not necccesery in Jupyter\n",
    "\n",
    "# 2) Objective:  #this one is more prefered\n",
    "# Create Figure (empty canvas)\n",
    "fig = plt.figure()\n",
    "# Add set of axes to figure\n",
    "axes = fig.add_axes([0.1, 0.1, 0.8, 0.8]) # left, bottom, width, height (range 0 to 1)\n",
    "# Plot on that set of axes\n",
    "axes.plot(x, y, 'b')\n",
    "axes.set_xlabel('Set X Label') # Notice the use of set_ to begin methods\n",
    "axes.set_ylabel('Set y Label')\n",
    "axes.set_title('Set Title')\n",
    "axes[2].set_ylim([0, 60])\n",
    "axes[2].set_xlim([2, 5])\n",
    "\n",
    "#3 create graph inside the graph\n",
    "axes1 = fig.add_axes([0.1, 0.1, 0.8, 0.8]) # main axes\n",
    "axes2 = fig.add_axes([0.2, 0.5, 0.4, 0.3]) # inset axes\n",
    "\n",
    "#Subplots in Objective:\n",
    "# Empty canvas of 1 by 2 subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "axes[0].plot(x, y, 'r')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes.set_title('title');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#style:\n",
    "liewidth == lw\n",
    "linestyle == ls\n",
    "ax.plot(x, x+1, color=\"blue\", alpha=0.5) # half-transparant\n",
    "ax.plot(x, x+16, color=\"purple\", lw=1, ls='-', marker='s', markersize=8, \n",
    "        markerfacecolor=\"yellow\", markeredgewidth=3, markeredgecolor=\"green\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig.savefig(\"filename.png\", dpi=200)  #dpi optiona, can choos any type of graphic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common issue with matplolib is overlapping subplots or figures. We ca use **fig.tight_layout()** or **plt.tight_layout()** method, which automatically adjusts the positions of the axes on the figure canvas so that there is no overlapping content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# subplot with 1 row, 2 columns, and current axis is 1st subplot axes\n",
    "#counting is from left to rigth, ant top to bottom\n",
    "plt.subplot(1, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.plot(x, x**2, label=\"x**2\")\n",
    "ax.plot(x, x**3, label=\"x**3\")\n",
    "ax.legend(loc=0)  #0 coresponds to \"best location\"\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.imshow(im);\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "Plotting 2D:     \n",
    "smotted data - image like, but not real values at all positions\n",
    "imshow(pca.components_[i].reshape((50, 37)), cmap=plt.cm.bone)\n",
    "separated values (no smoothing between data - real points)\n",
    "plt.matshow(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels);\n",
    "\n",
    "plt.plot(X_train, y_train, 'o', label=\"data\")\n",
    "plt.plot(X_train, y_pred_train, 'o', label=\"prediction\")\n",
    "plt.plot([X.min(), X.max()], [min_pt, max_pt], label='fit')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.plot(range(1, 11), distortions, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(a), importances[indices],\n",
    "       color=\"r\",  yerr = std[indices], align=\"center\")\n",
    "feat =  [keys[i] for i in indices]\n",
    "plt.xticks(range(a), feat)\n",
    "plt.xlim([-1, a])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subplots:\n",
    "plt.subplot(1,2,1) # raws,clumns, active\n",
    "plt.plot(X,Y) #then change the active to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = dta.groupby(\"education\").size().plot(kind=\"barh\", figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#heat map, but with given number of bins -> 2D histogram:\n",
    "plt.figure()\n",
    "_ = plt.hist2d(X, Y, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nice graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from seaborn import set_style\n",
    "set_style(\"darkgrid\")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slider widget (ipywidgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#collections of Jupyter widgets at:\n",
    "# http://ipywidgets.readthedocs.io/en/stable/examples/Widget%20List.html\n",
    "import ipywidgets as widgets\n",
    "\n",
    "widgets.FloatSlider(\n",
    "    value=7.5,\n",
    "    min=0,\n",
    "    max=10.0,\n",
    "    step=0.1,\n",
    "    description='Test:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.1f',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://ipywidgets.readthedocs.io/en/stable/examples/Using%20Interact.html\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "t = np.arange(0,1,0.01)\n",
    "\n",
    "def pltsin(f_my):\n",
    "    plt.plot(t, np.sin(2*np.pi*t*f_my))\n",
    "    plt.show()\n",
    "    \n",
    "interact(pltsin, f_my=(1,10,0.1)) #if such 3 elemets tuple provided, then widghet if FloatSlider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slider widget (matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Slider, Button, RadioButtons\n",
    "%matplotlib notebook\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,3))\n",
    "plt.subplots_adjust(left=0.25, bottom=0.25)\n",
    "t = np.arange(0.0, 1.0, 0.001)\n",
    "l = plt.plot(t, np.sin(20*t), lw=2, color='red')\n",
    "plt.axis([0, 1, -1, 1])\n",
    "\n",
    "axfreq = plt.axes([0.25, 0.1, 0.65, 0.06], facecolor='blue')\n",
    "samp = Slider(axfreq, 'Freq', 0.1, 40, 20)\n",
    "\n",
    "def update(val):\n",
    "    freq = samp.val\n",
    "    l[0].set_ydata(np.sin(freq*t))\n",
    "    fig.canvas.draw_idle()\n",
    "    fig.suptitle('Frequency f={:.1f}'.format(freq))\n",
    "\n",
    "samp.on_changed(update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animations and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as  np\n",
    "%matplotlib notebook\n",
    "n = 1000\n",
    "frames = 20\n",
    "x = np.random.randn(n)\n",
    "\n",
    "# create the function that will do the plotting, where curr is the current frame\n",
    "def update(curr):\n",
    "    plt.cla()\n",
    "    bins = np.arange(-4, 4, 0.5)\n",
    "    plt.hist(x[:int((curr+1)/frames*n)], bins=bins)\n",
    "    plt.axis([-4,4,0,250])\n",
    "    plt.gca().set_title('Sampling the Normal Distribution')\n",
    "    plt.gca().set_ylabel('Frequency')\n",
    "    plt.gca().set_xlabel('Value')\n",
    "    plt.annotate('n = {}'.format((curr+1)/frames*n), [-3,180])\n",
    "    \n",
    "fig = plt.figure()\n",
    "fig.patch.set_alpha(1)  # To make the bacgorund color not transparent!\n",
    "a = animation.FuncAnimation(fig, update, interval=100, frames=frames, repeat=False)\n",
    "\n",
    "##Saving to gif file:\n",
    "# First inastall ImageMagick, wwith Legacy instalation option on during instalation, to have a converter.exe file!\n",
    "# plt.rcParams['animation.convert_path'] = 'C:\\Program Files\\ImageMagick-7.0.7-Q16\\convert.exe'\n",
    "# anim.save('animation.gif', writer='imagemagick', fps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "x=np.linspace(0, 2, 20)\n",
    "y=x**2\n",
    "plt.plot(x,y)\n",
    "def onclick(event):\n",
    "    plt.gca().set_title('y={:.1f}'.format(event.ydata))\n",
    "plt.gcf().canvas.mpl_connect('button_press_event', onclick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as  np\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "%matplotlib notebook\n",
    "\n",
    "origins = ['China', 'Brazil', 'India', 'USA', 'Canada', 'UK', 'Germany', 'Iraq', 'Chile', 'Mexico']\n",
    "shuffle(origins)\n",
    "df = pd.DataFrame({'height': np.random.rand(10),\n",
    "                   'weight': np.random.rand(10),\n",
    "                   'origin': origins})\n",
    "plt.figure()\n",
    "# picker=5 means the mouse doesn't have to click directly on an event, but can be up to 5 pixels away\n",
    "plt.scatter(df['height'], df['weight'], picker=5)\n",
    "plt.gca().set_ylabel('Weight')\n",
    "plt.gca().set_xlabel('Height')\n",
    "\n",
    "def onpick(event):\n",
    "    origin = df.iloc[event.ind[0]]['origin']\n",
    "    plt.gca().set_title('Selected item came from {}'.format(origin))\n",
    "\n",
    "# tell mpl_connect we want to pass a 'pick_event' into onpick when the event is detected\n",
    "plt.gcf().canvas.mpl_connect('pick_event', onpick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving image and transparency issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig.patch.set_alpha(1)  # To make the bacgorund color not transparent!\n",
    "plt.savefig('demo.png', facecolor=fig.get_facecolor(), dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sns.jonintplot(2D distribution _ mariginials as additional grpahs)\n",
    "#sns.pairplot (n x n  grid of pair plots of the dataframe, + on diaglonal -> distributions)\n",
    "#sns.distplot\n",
    "#sns.lmplot # linear plot with linear regression fit \n",
    "ax = sns.distplot(x, hist_kws=dict(edgecolor=\"k\", linewidth=2))\n",
    "sns.set_palette(\"GnBu_d\")\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "#hue = 'Age' is nice parameter of some plot, that separates the data int sub categories - nice for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scatter plot-> by using lmplot and switching fitting to off:\n",
    "sns.lmplot(data = df, x='Room.Board', y='Grad.Rate', fit_reg=False, hue = 'Private')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## FacetGrid: composer of multiple plots:\n",
    "\n",
    "g = sns.FacetGrid(df, hue='Private', size=4, aspect=2)\n",
    "g = g.map(plt.hist, 'Outstate', alpha = 0.5, bins=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlines in his-like plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "As part of the update to matplotlib 2.0 the edges on bar plots are turned off by default. However, you may use the rcParam\n",
    "\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "\n",
    "to turn the edges on globally.\n",
    "\n",
    "Probably the easiest option is to specifically set the edgecolor when creating a seaborn plot, using the hist_kws argument,\n",
    "\n",
    "ax = sns.distplot(x, hist_kws=dict(edgecolor=\"k\", linewidth=2))\n",
    "\n",
    "For matplotlib plots, you can directly use the edgecolor or ec argument.\n",
    "\n",
    "plt.bar(x,y, edgecolor=\"k\")\n",
    "plt.hist(x, edgecolor=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotply - interactive plots\n",
    "could be also placed on website\n",
    "\n",
    "http://rickyreusser.com/plotly-expression-transform/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()\n",
    "df = pd.DataFrame(np.random.randn(100,4),columns='A B C D'.split())\n",
    "#and then istead of df.plot -> df.iplot!!!\n",
    "df.iplot(kind='scatter',x='A',y='B',mode='markers',size=10, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Types in Panda buil-in\n",
    "\n",
    "There are several plot types built-in to pandas, most of them statistical plots by nature:\n",
    "\n",
    "* df.plot.area     \n",
    "* df.plot.barh     \n",
    "* df.plot.density  \n",
    "* df.plot.hist     \n",
    "* df.plot.line     \n",
    "* df.plot.scatter\n",
    "* df.plot.bar      \n",
    "* df.plot.box      \n",
    "* df.plot.hexbin   \n",
    "* df.plot.kde      \n",
    "* df.plot.pie\n",
    "\n",
    "You can also just call df.plot(kind='hist') or replace that kind argument with any of the key terms shown in the list above (e.g. 'box','barh', etc..)\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "ax = plt.axes(projection='3d')\n",
    "xgrid, ygrid = np.meshgrid(x, y.ravel())\n",
    "ax.plot_surface(xgrid, ygrid, im, cmap=plt.cm.jet, cstride=2, rstride=2, linewidth=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = y[:, np.newaxis] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#map - can use dict to change values accoring to dict\n",
    "map(function, iterable, ...)\n",
    "Apply function to every item of iterable and return a list of the results. If additional iterable \n",
    "arguments are passed, function must take that many arguments and is applied to the items \n",
    "from all iterables in parallel. \n",
    "\n",
    "#list comperhension\n",
    "[item**2 for item in x]\n",
    "[item**2 for item in x if x<2]\n",
    "\n",
    "[x+1 if x >= 45 else x+5 for x in l]\n",
    "#map -apply to each element\n",
    "list(map(lambda var: var*2,seq))\n",
    "#filter, select elements\n",
    "list(filter(lambda item: item%2 == 0,seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sorting by vale (or second element) in dict (similar in tuple or list):\n",
    "import operator\n",
    "Sortedby2element = sorted(dictionary.items(), key=operator.itemgetter(1), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=range(2,30,3)\n",
    "b=a.count\n",
    "b(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#argsort() - returns indices of array after sorting (not vlues)\n",
    "sorted_coef_index = model.coef_[0].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 4, 620, 519, 382]\n",
      "[840, 197, 979, 872, 423]\n"
     ]
    }
   ],
   "source": [
    "#unpocking tuple:\n",
    "aaa=[(6, 840), (4, 197), (620, 979), (519, 872), (382, 423)]\n",
    "bb, cc = zip(*aaa)\n",
    "print(list(bb)[:5])\n",
    "print(list(cc)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#matrix multipication\n",
    "# np.dot calls matrix-matrix multiplication while * is element wise multiplication. \n",
    "# The symbol for matrix-matrix multiplication is @ for Python 3.5+."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://docs.scipy.org/doc/numpy-dev/user/quickstart.html\n",
    "\n",
    "np.bincount(iris.target)\n",
    "data.shape\n",
    "print(np.all(Arra1 == digits.data))\n",
    "correct_idx = np.where(pred_y == test_y)[0]\n",
    "x = np.linspace(-3, 3, 100)\n",
    "np.isnan(data).any()\n",
    "np.set_printoptions(precision=2)\n",
    "np.set_printoptions(suppress=True)  # to show 1e-15 (numeric zero as 0)\n",
    "np.array(my_matrix)\n",
    "np.arange(0,11,2)\n",
    "np.linspace(0,10,3)\n",
    "np.logspace(-3,3,6)\n",
    "np.random.rand(5,5) #unifrom distr\n",
    "np.random.randn(5,5) #normal (gaussian distr)\n",
    "np.random.randint(5,5) #intigers, low, high, no. of samles\n",
    "array.argmax() - gives locationof max vale\n",
    "arr.shape  - without parenthsis, its attrribute not method\n",
    "arr.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Broadcasting\n",
    "import numpy as np\n",
    "arr = np.arange(0,11)\n",
    "arr_sub = arr[2:5]\n",
    "arr_sub[:]= 10\n",
    "print(arr)\n",
    "#but if \n",
    "arr_sub = 20\n",
    "print(arr)\n",
    "#or\n",
    "arr_ele = arr[5:6]\n",
    "print(arr_ele)\n",
    "arr_ele[:] = np.array([99])\n",
    "print(arr)\n",
    "# but this will not propagate\n",
    "arr_ele = np.array([77])\n",
    "print(arr)\n",
    "#To get a copy, need to be explicit\n",
    "arr_copy = arr.copy()\n",
    "\n",
    "#     B=A creates a reference\n",
    "#     B[:]=A makes a deep copy\n",
    "#     numpy.copy(B,A) makes a copy\n",
    "\n",
    "# the last two need additional memory.\n",
    "# When you are doing\n",
    "# b=A[:,1]\n",
    "# it is creating a reference to the underlying array. But in this case\n",
    "# A[:,0] = b\n",
    "# only values are copied.\n",
    "\n",
    "#Fancy slicing, every n-th eelemnt\n",
    "arr1=np.arange(20)\n",
    "print(arr1)\n",
    "arr2=arr1[1:16:2]\n",
    "print(arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr1=np.arange(10)\n",
    "boolarr=arr1>5\n",
    "boolarr\n",
    "arr2=arr1[boolarr]\n",
    "arr2\n",
    "# or:\n",
    "arr1[arr1>5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#universal functions;\n",
    "https://docs.scipy.org/doc/numpy/reference/ufuncs.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# matrix muutiplication\n",
    "np.dot(x, y)\n",
    "OR:\n",
    "In python 3.5, the @ operator was introduced for matrix multiplication, following PEP465. \n",
    "This is implemented e.g. in numpy as the matmul operator.\n",
    "But @ would throw an errror when called for scalars!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('precision', 5)\n",
    "pd.set_option(\"display.max_rows\", 16)  #to set max size of ploted dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nice info summary of DF:\n",
    "df1.info()\n",
    "df1.describe()\n",
    "df.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.read_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Positively Rated'] = np.where(df['Rating'] > 3, 1, 0)\n",
    "spam_data['target'] = np.where(spam_data['target']=='spam',1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newdf = df[(df['col1']>2) & (df['col2']==444)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd   \n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LARGE_FIGSIZE = (12, 8) #size of matplotlib figures\n",
    "\n",
    "Or in single frame:\n",
    "with pd.option_context(\"max_rows\", 20):\n",
    "    print(education_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply with if condition:\n",
    "df['Cluster'] = df['Private'].apply(lambda x: 1 if x=='Yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for muliindexes nice slection using xs()\n",
    "DataFrame.xs(key, axis=0, level=None, drop_level=True)\n",
    "\n",
    "    Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. \n",
    "    Defaults to cross-section on the rows (axis=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dta.groupby(\"income\").education.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#uniqe valesu:\n",
    "df['col2'].nunique() #no. of uniqe vales\n",
    "OR .unique() #uniqe vvalues \n",
    "OR: .value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ecom['Email'].apply(lambda x: x.split('@')[1]).value_counts().head(5) #get most popular email providers\n",
    "# apply: similar to map in python list\n",
    "df['col1'].apply(times2) #own functino\n",
    "df['col3'].apply(len) #built in function\n",
    "df['col3'].apply(lambda x: x**3)  #with lambda expression\n",
    "\n",
    "#.agg()\n",
    "Some plausible advantages of using .agg() compared to .apply(), for DataFrame GroupBy objects would be:\n",
    "1) .agg() gives the flexibility of applying multiple functions at once, or pass a list of function to each column.\n",
    "2) Also, applying different functions at once to different columns of dataframe.\n",
    "In [276]: df.groupby([\"name\", \"score_1\"]).agg({\"score_3\" :[np.sum, np.min, np.mean, np.max], \"score_2\":lambda x : x.mean()})\n",
    "So, .agg( ) could be really handy at handling the DataFrameGroupBy objects, as compared to .apply( ). \n",
    "But, if you are handling only pure dataframe objects, and not DataFrameGroupBy objects then \n",
    "apply() can be very useful, as apply( ) can apply a function along any axis of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.sort_values('Col1')\n",
    "df.columns() #returns a list with columns names\n",
    "df.index() #retruns the indices\n",
    "df.isnull()#\n",
    "df.pivot_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assign vale to cell:\n",
    "data.loc[data['name'] == 'fred', 'A'] = 0\n",
    "insted of working on coppy of data:\n",
    "data[data['name']=='fred']['A']=0 @which owrks on copy of dataframe!!! Wrong\n",
    "\n",
    ".loc is faster, because it does not try to create a copy of the data.\n",
    ".loc is meant to modify your existing dataframe inplace, which is more memory efficient.\n",
    ".loc is predictable, it has one behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " [[len(i) for i in X_train], X_train.str.count(r'\\d'), X_train.str.count(r'\\W')])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot (pivot_table) - melt, stack - unstack\n",
    "Nice graphical explanantion:\n",
    "http://nikgrozev.com/2015/07/01/reshaping-in-pandas-pivot-pivot-table-stack-and-unstack-explained-with-pictures/\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reshaping.html\n",
    "pivot_table (instead of pivot) can handle duplicate data, that can occure during pivoing, and then make\n",
    "some aggregate function on those, line: mean, max, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pivot table, takes a velues from column, and creates a new columns out of those data\n",
    "#melt is oposite\n",
    "#unstact, takes the innermoste index and moves it up into the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pivot table\n",
    "import pandas as pd\n",
    "data = {'A':['foo','foo','foo','bar','bar','bar'],\n",
    "     'B':['one','one','two','two','one','one'],\n",
    "       'C':['x','y','x','y','x','y'],\n",
    "       'D':[1,3,2,5,4,1]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.pivot_table(values='D',index=['A', 'B'],columns=['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Install package of extensions from: http://jupyter-contrib-nbextensions.readthedocs.io\n",
    "(autosave, Table of content 2)\n",
    "\n",
    "conda install -c conda-forge jupyter_contrib_nbextensions'\n",
    "jupyter contrib nbextension install --user\n",
    "\n",
    "then eneble the manager - usefull hepler of extensions:\n",
    "https://github.com/Jupyter-contrib/jupyter_nbextensions_configurator\n",
    "\n",
    "jupyter nbextensions_configurator enable --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%timeit #-n<N>: execute the given statement <N> times in a loop. If this value\n",
    "is not given, a fitting value is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shith-tab on parentehsis to give hepl on the function (cirulate clicking for more info) or function with ? at the end\n",
    "\n",
    "#after object and its dot you can clik tab, to see its methods:)A\n",
    "# alt+enter -run cell and insert new cell below\n",
    "#for blockcomment : CTR+/ for uncomment\n",
    "#for indentation CTR+] or CTR+["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load py script:\n",
    "%load solutions/file.py\n",
    "or: %load solutions/file (without .py)\n",
    "%%writefile filename.py\t\n",
    "%run myfile.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#insterting images:\n",
    "<img src=\"ExampleImageCheetshet.jpg\" width=300 />\n",
    "\n",
    "OR:\n",
    "![Alternative text that describes the graphic](ExampleImageCheetshet.jpg \"Optional title over mouse\")\n",
    "#include reference to image\n",
    "#may add some caption:\n",
    "<h4 align=\"center\">Figure X: Caption for image</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Bookmarks an refernces:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: [Click here](#my_ref1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "->Here goes the reference<a id='my_ref1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loading the numpy data\n",
    "mean = np.load(os.path.join(DATA_JOB_DIR,'train_db','mean.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow  - use KERAS API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow with ContribLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# should be working fit TF v1.0\n",
    "import tensorflow.contrib.learn as learn\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=1)]\n",
    "classifier = learn.DNNClassifier(n_classes=2, hidden_units=[10, 20, 10], feature_columns=feature_columns)\n",
    "\n",
    "classifier.fit(X_train, y_train, steps=200, batch_size=32)\n",
    "iris_predictions = classifier.predict(X_test, as_iterable=False)\n",
    "print(classification_report(y_test,iris_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Hello world with TF:\n",
    "import tensorflow as tf\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.learn as skflow\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets, metrics, preprocessing\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=1)]\n",
    "X = preprocessing.StandardScaler().fit_transform(boston.data)\n",
    "regressor = skflow.LinearRegressor(feature_columns=feature_columns)\n",
    "regressor.fit(X, boston.target, steps=2000)\n",
    "# score = metrics.mean_squared_error(regressor.predict(X), boston.target)\n",
    "# print (\"MSE: %f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.learn as skflow\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import datasets, metrics\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=1)]\n",
    "classifier = skflow.DNNClassifier(feature_columns= feature_columns ,hidden_units=[2, 2, 2], n_classes=3)\n",
    "classifier.fit(iris.data, iris.target, steps=200)\n",
    "accuracy = tf.reduce_mean(tf.cast(iris.target, \"float\"))\n",
    "print(\"Accuracy:\", accuracy(, y: test_labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usefull labraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas_datareader import data, wb # for reading stocks data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/\n",
    "example with included jupyter codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {
    "height": "121px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "532px",
    "left": "0px",
    "right": "1155.33px",
    "top": "107px",
    "width": "178px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
